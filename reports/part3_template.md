# Predictions vs Reality: How Well Did We Do?

*Published within 48 hours of May 7th 2026 local election results.*

---

**Model frozen 30 April 2026 to prevent adaptive tuning. No parameters, scenarios, or uncertainty bands were modified after this timestamp.**

---

## Summary

[FILL: 2-paragraph summary of overall accuracy]

## Methodology Reminder

[FILL: Brief restatement of what the model was doing — volatility metrics, not seat predictions]

## Accuracy by Metric

[FILL: MAE per metric per tier — output of audit_results.py]

## Interval Coverage

[FILL: % of actual results within P10–P90 bands per metric — target ~80%]

## Scenario Ranking

[FILL: Which scenario (S0–S5) best described what actually happened?]

## Where We Were Wrong

[FILL: Honest decomposition of errors. No post-hoc rationalisation.]

## What This Means for Future Iterations

[FILL: Key calibration insights for replication]

---

*civic-lens is open source. Fork the methodology, fork the calibration, build on it: [GitHub link]*
